# -*- coding: utf-8 -*-
"""WoC_6.0_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eMamI6TVKx1YsWpIgz-8zAD9Q2r-2qTM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.figure_factory as ff
from wordcloud import WordCloud
import random
import string
import tensorflow as tf

with open('/content/dataset.txt','r') as f:
  names = f.read().split("\n")[:-1]

print("Number of Names: ",len(names))
print("\nMax Length of a Name: ",max(map(len,names))-1)

names_size = len(names)
for i in range(10):
  a = random.randint(0, len(names))
  print(names[a])

MAX_LENGTH = 10
names = [name for name in names if len(name)<=MAX_LENGTH]
print("Number of Names: ",len(names))

assert max(map(len,names))<=MAX_LENGTH, f"Names length more than {MAX_LENGTH}"

start_token = " "

#padding
pad_token = "#"

#adding start token in front of all Names
names = [start_token+name for name in names]
MAX_LENGTH += 1

# set of tokens
tokens = sorted(set("".join(names + [pad_token])))

tokens = list(tokens)
n_tokens = len(tokens)
print("Tokens: ",tokens)
print ('n_tokens:', n_tokens)

ttd = dict(zip(tokens,range(len(tokens))))

def to_matrix(names, mx_ln=None, pad=ttd[pad_token], dtype=np.int32):
  mx_ln = mx_ln or max(map(len, names))
  names_ix = np.zeros([len(names), mx_ln], dtype) + pad

  for i in range(len(names)):
      name_ix = list(map(ttd.get, names[i]))
      names_ix[i, :len(name_ix)] = name_ix

  return names_ix

X = to_matrix(names)
X_train = np.zeros((X.shape[0],X.shape[1],n_tokens),np.int32)
y_train = np.zeros((X.shape[0],X.shape[1],n_tokens),np.int32)

for i, name in enumerate(X):
  for j in range(MAX_LENGTH-1):
      X_train[i,j,name[j]] = 1
      y_train[i,j,name[j+1]] = 1
  X_train[i,MAX_LENGTH-1,name[MAX_LENGTH-1]] = 1
  y_train[i,MAX_LENGTH-1,ttd[pad_token]] = 1

def make_model():
  model = tf.keras.models.Sequential()
  model.add(tf.keras.layers.Embedding(n_tokens,
                                      16,input_length=MAX_LENGTH))
  model.add(tf.keras.layers.SimpleRNN(256,
                                      return_sequences=True,
                                      activation='elu'))
  model.add(tf.keras.layers.SimpleRNN(256,
                                      return_sequences=True,
                                      activation='elu'))
  model.add(tf.keras.layers.Dense(n_tokens,
                                  activation='softmax'))
  model.compile(loss='categorical_crossentropy',
                optimizer=tf.keras.optimizers.Adam(0.001))
  return model
model = make_model()
model.summary()

name_count = X.shape[0]
BS = 32
STEPS_PER_EPOCH = np.ceil(name_count/BS)

AUTO = tf.data.experimental.AUTOTUNE
ignore_order = tf.data.Options()
ignore_order.experimental_deterministic = False
train_dataset = (
  tf.data.Dataset.from_tensor_slices((X,y_train))
  .shuffle(5000)
  .cache()
  .repeat()
  .batch(BS)
  .prefetch(AUTO))

class CyclicLR(tf.keras.callbacks.Callback):

  def __init__(self,base_learning_rate=1e-5,mx_learning_rate=1e-3,ss=10):
      super().__init__()

      self.base_learning_rate = base_learning_rate
      self.mx_learning_rate = mx_learning_rate
      self.ss = ss
      self.iterations = 0
      self.history = {}

  def clr(self):
      cycle = np.floor((1+self.iterations)/(2*self.ss))
      x = np.abs(self.iterations/self.ss - 2*cycle + 1)

      return self.base_learning_rate + (self.mx_learning_rate - self.base_learning_rate)*(np.maximum(0,1-x))

  def on_train_begin(self,logs={}):
      tf.keras.backend.set_value(self.model.optimizer.lr, self.base_learning_rate)

  def on_batch_end(self,batch,logs=None):
      logs = logs or {}

      self.iterations += 1

      self.history.setdefault('lr', []).append(tf.keras.backend.get_value(self.model.optimizer.lr))
      self.history.setdefault('iterations', []).append(self.iterations)

      for k, v in logs.items():
          self.history.setdefault(k, []).append(v)

      tf.keras.backend.set_value(self.model.optimizer.lr, self.clr())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# cyclicLR = CyclicLR()
# EPOCHS = 50
# history = model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=EPOCHS,callbacks=[cyclicLR])
# 
# model.save('saved_model.h5')

plot = go.Figure()
plot.add_trace(go.Scatter(x=np.arange(1,len(history.history['loss'])+1), y=history.history['loss'], mode='lines+markers', name='Training loss'))
plot.update_layout(title_text="Training loss")
plot.show()

def generateName(model=model, sp=start_token, mxl=MAX_LENGTH):
  assert len(sp)<mxl, f"Length of the Seed-phrase is more than Max-Length: {mxl}"
  name = [sp]
  x = np.zeros((1,mxl),np.int32)
  x[0,0:len(sp)] = [ttd[token] for token in sp]
  for i in range(len(sp),mxl):
      p = list(model.predict(x)[0,i-1])
      p = p/np.sum(p)
      index = np.random.choice(range(n_tokens),p=p)
      if index == ttd[pad_token]:
          break
      x[0,i] = index
      name.append(tokens[index])
  return "".join(name)

weights = 'IndianNamesWeights.h5'
model.save_weights(weights)

predictor = make_model()
predictor.load_weights(weights)

import numpy as np
import string

sp = f" {np.random.choice(list(string.ascii_uppercase))}"
generated_names = []

for _ in range(200):
    name = generateName(predictor, sp=sp)
    if name not in names:
        generated_names.append(name.lstrip())
    else:
        generated_names.append(name.lstrip())

# Writing generated names to a file
with open("new_names.txt", "w") as file:
    for generated_name in generated_names:
        file.write(f"{generated_name}\n")

print("Generated names have been saved to new_names.txt")

#finding uniqueness %
existing_names = set(names)
generated_names = set(generated_names)

common_names = existing_names.intersection(generated_names)

# Calculating uniqueness percentage
uniqueness_percentage = (len(generated_names) - len(common_names)) / len(generated_names) * 100

print(f"Uniqueness percentage: {uniqueness_percentage:.2f}%")

